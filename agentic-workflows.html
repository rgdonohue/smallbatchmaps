<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Agentic Development Case Study - Small Batch Maps</title>
    <meta name="description" content="Complete case study on building production AI-assisted geospatial software with the Tilecraft project.">
    
    <!-- Font optimization -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;600;700&family=Inter:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
    
    <!-- Custom CSS -->
    <link rel="stylesheet" href="styles.css">
</head>
<body class="case-study-body">
    <!-- Fixed Header -->
    <header class="header" role="banner">
        <nav class="nav" role="navigation" aria-label="Main navigation">
            <div class="logo">
                <a href="index.html" class="logo-link visible" aria-label="Small Batch Maps - Go to homepage">
                    <img src="images/logo-point.png" alt="Small Batch Maps compass logo" class="logo-image visible" loading="eager" />
                    Small Batch Maps
                </a>
            </div>
            
            <!-- Desktop Navigation -->
            <ul class="nav-desktop" role="menubar">
                <li role="menuitem"><a href="index.html#services" class="nav-link">Services</a></li>
                <li role="menuitem"><a href="index.html#about" class="nav-link">About</a></li>
                <li role="menuitem"><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li role="menuitem"><a href="index.html#contact" class="nav-link">Contact</a></li>
            </ul>
            
            <!-- Mobile Menu Button -->
            <button id="mobile-menu-btn" class="mobile-menu-btn" aria-label="Toggle mobile menu" aria-expanded="false" aria-controls="mobile-menu">
                <div class="hamburger">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </button>
        </nav>
        
        <!-- Mobile Navigation -->
        <div id="mobile-menu" class="mobile-menu hidden" role="menu" aria-labelledby="mobile-menu-btn">
            <ul class="mobile-nav-list" role="none">
                <li role="menuitem"><a href="index.html#services" class="nav-link">Services</a></li>
                <li role="menuitem"><a href="index.html#about" class="nav-link">About</a></li>
                <li role="menuitem"><a href="index.html#projects" class="nav-link">Projects</a></li>
                <li role="menuitem"><a href="index.html#contact" class="nav-link">Contact</a></li>
            </ul>
        </div>
    </header>



    <!-- Navigation -->
    <nav class="case-study-nav">
        <div class="container">
            <div class="nav-links">
                <a href="#part1">Introduction</a>
                <a href="#part2">Architecture</a>
                <a href="#part3">Testing</a>
                <a href="#part4">Technical Deep-Dive</a>
                <a href="#part5">Recommendations</a>
                <a href="#slides">Executive Summary</a>
            </div>
        </div>
    </nav>

    <!-- Case Study Content -->
    <main class="case-study-content">
        <!-- Part 1: Introduction -->
        <section id="part1" class="part-section">
            <div class="part-header">
                <div class="part-number">Part 1</div>
                <h2 class="part-title">Agentic Development in the Wild</h2>
            </div>
            <div class="part-content">
                <p>This project was purpose-built to study the emerging practice of agentic development—integrating AI assistance throughout the software lifecycle. The objective wasn't just to ship a working CLI tool, but to rigorously evaluate how modular architectures, shared configurations, and resilient design can support meaningful collaboration with large language models (LLMs).</p>
                
                <p>The result was <strong>Tilecraft</strong>, a production-ready vector tile generator built with an AI-augmented workflow. Along the way, we codified concrete architectural patterns, documented code and test metrics, and assessed real-world LLM capabilities and limitations through direct observation.</p>

                <h3>Context: Building Tilecraft</h3>
                <p>Tilecraft ingests OSM data, transforms it into a vector tile schema, and renders it in a custom cartographic style using open standards. At its core, it's a CLI app built to be fast, modular, and human-friendly.</p>

                <blockquote>
                    Agentic development refers to a modular, AI-augmented workflow that integrates LLM agents as co-workers across the software lifecycle—from design and config generation to real-time code fixes and test authoring.
                </blockquote>

                <h3>Key Principles Discovered</h3>
                
                <h4>1. Modularize Everything (for the Agent, Not Just the Human)</h4>
                <p>AI tools work better with boundaries. We quickly learned that long monolithic files or loosely scoped functions led to poor LLM performance and lower-quality suggestions. Refactoring our codebase into tight, single-responsibility modules improved LLM interactions dramatically.</p>

                <h4>2. Configuration is Collaboration</h4>
                <p>We started with human-readable YAML configs for data sources, layer schemas, and styling parameters. But midway through, we realized that these configs also served as a shared memory space for the AI agents.</p>

                <h4>3. Use Progressive Enhancement, but for Intelligence</h4>
                <p>Rather than expect the AI to write perfect code the first time, we scaffolded workflows that let it propose partial solutions: write function shells with TODOs, auto-insert logging wrappers, fallback to mocked outputs when real data fails.</p>

                <h4>4. Graceful Degradation is Not Optional</h4>
                <p>LLMs will get things wrong. They will hallucinate field names, misinterpret schemas, or propose unparseable JSON. The key is to design with failure as a first-class citizen.</p>

                <h3>Documented Results</h3>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <span class="metric-value">4,632</span>
                        <span class="metric-desc">Lines of production code</span>
                    </div>
                    <div class="metric-card">
                        <span class="metric-value">27</span>
                        <span class="metric-desc">Automated tests written</span>
                    </div>
                    <div class="metric-card">
                        <span class="metric-value">64%</span>
                        <span class="metric-desc">Test coverage achieved</span>
                    </div>
                    <div class="metric-card">
                        <span class="metric-value">100%</span>
                        <span class="metric-desc">Pipeline functionality</span>
                    </div>
                </div>

                <h3>Qualitative Observations</h3>
                <p>While we lack baseline measurements for precise quantification, the development process exhibited several notable characteristics:</p>
                <ul>
                    <li><strong>Development velocity felt accelerated</strong> compared to similar past projects, though without controlled measurement</li>
                    <li><strong>AI generated substantial boilerplate code</strong> for CLI scaffolding, error handling, and documentation</li>
                    <li><strong>Test-first approach reduced iteration cycles</strong> by catching AI-generated errors early</li>
                    <li><strong>Rich documentation was AI-assisted</strong> but required human review and refinement</li>
                    <li><strong>Error handling was more comprehensive</strong> due to AI's systematic approach to edge cases</li>
                </ul>
            </div>
        </section>

                 <!-- Part 2: Architecture Patterns -->
         <section id="part2" class="part-section">
             <div class="part-header">
                 <div class="part-number">Part 2</div>
                 <h2 class="part-title">Architecture Patterns for Human-AI Collaboration</h2>
             </div>
             <div class="part-content">
                 <p>Agentic development demands more than clever prompting. It requires intentional system design that accommodates partial automation, graceful failure, and clear task boundaries.</p>

                 <h3>1. Dual Loop Architecture</h3>
                 <p>We adopted a dual feedback loop model: one loop for human-driven development, and another for AI-generated artifacts. Both loops interacted with a shared config state and logging system.</p>

                 <h3>2. Structured Prompts via YAML Templates</h3>
                 <p>Rather than write prompts ad hoc, we stored them as parameterized YAML templates. These templates could be programmatically populated with context and included a comment field for human notes.</p>

                 <h3>3. Agent-Writable Configs with Human Locks</h3>
                 <p>We used config files as the shared working memory for the system—but guarded critical fields with human locks. LLMs could read and suggest edits, but flagged changes for human approval.</p>

                 <h3>4. Self-Scaffolding CLI Interface</h3>
                 <p>We structured the CLI to emit AI-scaffoldable hints by default. Running <code>tilecraft build</code> would check for missing configs, emit markdown-style prompt suggestions if failure occurred, and offer retry with <code>--with-agent</code> to auto-fill missing elements.</p>

                 <h3>5. Observability Layer for AI Behavior</h3>
                 <p>We added structured logging for prompt content and AI responses, execution time and error rates by agent task, and human overrides and final decisions. All logs were indexed and queryable via a local dashboard.</p>
             </div>
         </section>

         <!-- Part 3: Testing -->
         <section id="part3" class="part-section">
             <div class="part-header">
                 <div class="part-number">Part 3</div>
                 <h2 class="part-title">Testing and Quality Assurance for AI-Enhanced Systems</h2>
             </div>
             <div class="part-content">
                 <p>Testing AI-generated code adds complexity. But with deliberate patterns and workflows, we maintained stability across the pipeline and enforced standards that AI alone wouldn't catch.</p>

                 <h3>What We Actually Did</h3>
                 <ul>
                     <li>Wrote <strong>27 automated tests</strong> targeting critical path components</li>
                     <li>Achieved <strong>64% test coverage</strong> on production modules</li>
                     <li>Implemented <strong>test-first design</strong> for many agentic features</li>
                     <li>Used LLMs to <strong>generate and explain</strong> tests—but always with human review</li>
                     <li>Built a lightweight <strong>QA checklist</strong> for verifying AI-assisted pull requests</li>
                 </ul>

                 <h3>Key Testing Patterns</h3>

                 <h4>Pattern 1: Test the Boundaries, Not Just the Happy Path</h4>
                 <p>AI often generates plausible-looking code—but it's the edge cases where things break. We adopted a testing approach that emphasized boundary conditions: missing input files, unexpected schema fields, invalid config parameters, network timeouts.</p>

                 <h4>Pattern 2: Generate Test Skeletons, Then Refactor</h4>
                 <p>We frequently asked the AI to write test scaffolds. The results were hit-or-miss—but still useful for structure, fixtures, and naming. But we always reviewed the logic and added context-specific assertions.</p>

                 <h4>Pattern 3: Validate Prompt Chains and Agent Steps</h4>
                 <p>Many components relied on prompt-chain logic. We tested these by running dry simulations, capturing intermediate outputs, and validating schema consistency across steps.</p>

                 <h4>Pattern 4: Use Logging as Test Evidence</h4>
                 <p>Some AI-generated behavior wasn't easily testable with asserts. Instead, we relied on structured logs as verification: was the right fallback invoked? Did the agent use the correct schema template?</p>

                 <h4>Pattern 5: Human-in-the-Loop QA Checklist</h4>
                 <p>We adopted a simple checklist for reviewing agent-generated pull requests: CLI functionality, fallback behavior, realistic sample testing, prompt readability, and schema contract compliance.</p>
             </div>
         </section>

         <!-- Part 4: Technical Deep-Dive -->
         <section id="part4" class="part-section">
             <div class="part-header">
                 <div class="part-number">Part 4</div>
                 <h2 class="part-title">From 0 to Production: Technical Deep-Dive</h2>
             </div>
             <div class="part-content">
                 <p>This section offers a granular look at what it actually took to go from zero to production with an AI-augmented workflow focused on data science infrastructure—from ingestion to export.</p>

                 <h3>The Stack</h3>
                 <ul>
                     <li><strong>Python (3.11)</strong> for all pipeline components</li>
                     <li><strong>Pydantic</strong> for data validation and configuration schemas</li>
                     <li><strong>FastAPI</strong> for exposing processing steps as services</li>
                     <li><strong>Tippecanoe</strong> and <strong>tilemaker</strong> for vector tile generation</li>
                     <li><strong>DuckDB</strong> and <strong>GeoPandas</strong> for local data handling and ETL</li>
                     <li><strong>Jinja2</strong> for templating prompts and config files</li>
                     <li><strong>Rich</strong> and <strong>Typer</strong> for building a modern CLI</li>
                 </ul>

                 <h3>Pipeline Overview</h3>
                 <p>The data science workflow had five major stages:</p>
                 
                 <ol>
                     <li><strong>Data Ingestion:</strong> Download and stage OSM extracts, convert to GeoJSON or FlatGeobuf, extract relevant features</li>
                     <li><strong>Schema Mapping:</strong> Normalize attributes across layers, use LLM to propose style schemas, validate with Pydantic</li>
                     <li><strong>Tiling + Caching:</strong> Use Tippecanoe to generate .mbtiles, apply caching logic, compress outputs</li>
                     <li><strong>Export + Serving:</strong> Host tiles locally or upload to S3-compatible storage</li>
                     <li><strong>Documentation:</strong> Auto-generate Markdown docs, create zipped bundles with README and logs</li>
                 </ol>

                 <h3>Where the AI Helped</h3>
                 <ul>
                     <li><strong>Schema proposals:</strong> GPT-4 reliably generated proposed JSON schema mappings for common OSM tags</li>
                     <li><strong>Error handling templates:</strong> AI helped generate boilerplate try/except scaffolds</li>
                     <li><strong>Markdown documentation:</strong> Agent summarized config contents and output formats</li>
                     <li><strong>Naming + Refactoring:</strong> AI-suggested class and function names following role-based prompts</li>
                 </ul>

                 <h3>Where the AI Struggled</h3>
                 <ul>
                     <li><strong>Complex conditional logic:</strong> Multi-field exceptions made AI suggestions brittle</li>
                     <li><strong>Multi-file context:</strong> Without clear scaffolding, LLMs lost track of dependencies</li>
                     <li><strong>Caching and fingerprinting:</strong> Schema-aware deduplication had to be hand-coded</li>
                 </ul>

                 <div class="metrics-grid">
                     <div class="metric-card">
                         <span class="metric-value">4,632</span>
                         <span class="metric-desc">Lines of Code across 9 modules</span>
                     </div>
                     <div class="metric-card">
                         <span class="metric-value">27</span>
                         <span class="metric-desc">Tests with 64% coverage</span>
                     </div>
                     <div class="metric-card">
                         <span class="metric-value">100%</span>
                         <span class="metric-desc">Outputs versioned with logs</span>
                     </div>
                 </div>
             </div>
         </section>

         <!-- Part 5: Recommendations -->
         <section id="part5" class="part-section">
             <div class="part-header">
                 <div class="part-number">Part 5</div>
                 <h2 class="part-title">Reflections + Recommendations: The Agentic Playbook</h2>
             </div>
             <div class="part-content">
                 <p>This final piece distills what we've learned into a practical playbook for developers and teams experimenting with AI-augmented workflows. This is not a hype piece—it's a synthesis grounded in actual practices, observations, and limitations.</p>

                 <h3>🧭 Core Principles</h3>
                 <ol>
                     <li><strong>AI Is a Tool, Not a Teammate:</strong> Treat it like a powerful code-generation utility—not a replacement for design judgment, testing, or architecture.</li>
                     <li><strong>Agentic Workflows Require Structure:</strong> Modularize code, externalize configs, and version prompts. Reproducibility beats cleverness.</li>
                     <li><strong>Human-in-the-Loop Is Not Optional:</strong> Every LLM-generated artifact must pass through a human QA checkpoint.</li>
                     <li><strong>Failures Are Features, If You Design for Them:</strong> Expect errors, hallucinations, and ambiguity. Build fallback logic in from the start.</li>
                     <li><strong>Documentation Is Part of the System:</strong> Use AI to help write it—but make documentation part of the workflow.</li>
                 </ol>

                 <h3>🧱 What to Build</h3>
                 <div style="background: var(--cream); padding: 2rem; border-radius: 0.75rem; margin: 1.5rem 0;">
                     <h4 style="color: var(--teal); margin-top: 0;">Build These:</h4>
                     <ul>
                         <li>✅ Config schemas that AI can both read and write</li>
                         <li>✅ A CLI that emits helpful error states and scaffoldable prompts</li>
                         <li>✅ A prompt engine with context injection and variable substitution</li>
                         <li>✅ Logging utilities that include AI provenance</li>
                         <li>✅ Agent-aware test harnesses and fallback scripts</li>
                     </ul>
                     
                     <h4 style="color: var(--terracotta);">Avoid These:</h4>
                     <ul>
                         <li>❌ Opaque prompt chains with no logging</li>
                         <li>❌ Storing LLM outputs in production without human review</li>
                         <li>❌ AI agents with access to destructive file operations</li>
                     </ul>
                 </div>

                 <h3>🧩 Where to Start</h3>
                 <p>If you're just beginning to explore agentic development:</p>
                 <ol>
                     <li>Pick a simple CLI-based tool you want to build</li>
                     <li>Define the core config schema and user inputs</li>
                     <li>Write out your AI prompts as YAML templates</li>
                     <li>Use AI to scaffold, but manually test everything</li>
                     <li>Add structured logging, doc generation, and fallback behavior</li>
                 </ol>

                 <blockquote>
                     The future of AI-augmented software development will not be about autonomous agents building software alone. It will be about designing systems where humans and AI interact fluidly, accountably, and effectively.
                 </blockquote>
             </div>
         </section>

         <!-- Executive Summary (Slides) -->
         <section id="slides" class="part-section">
             <div class="part-header">
                 <div class="part-number">Executive Summary</div>
                 <h2 class="part-title">Key Takeaways & Presentation Outline</h2>
             </div>
             <div class="part-content">
                 <p>This executive summary captures the essential insights from the Tilecraft project for teams and decision-makers evaluating agentic development approaches.</p>

                 <h3>Why This Work Matters</h3>
                 <ul>
                     <li>Software development is changing: LLMs are becoming part of the standard toolchain</li>
                     <li>Developer teams need structured approaches, not just experimentation</li>
                     <li>Agentic development represents a measurable, reproducible pattern worth studying</li>
                 </ul>

                 <h3>What Is Agentic Development?</h3>
                 <ul>
                     <li><strong>Definition:</strong> Modular, AI-augmented workflows with human oversight</li>
                     <li><strong>Goal:</strong> Augment—not automate—the developer process</li>
                     <li><strong>Structure:</strong> Human ↔ Config ↔ AI feedback loops with shared state</li>
                 </ul>

                 <h3>Case Study Results: Tilecraft</h3>
                 <ul>
                     <li>CLI tool to convert OSM → Vector Tiles</li>
                     <li>Built from scratch using AI throughout the lifecycle</li>
                     <li>Emphasis on reproducibility, modularity, and testability</li>
                     <li><strong>Documented metrics:</strong> 4,632 LOC, 27 tests, 64% coverage</li>
                     <li><strong>Qualitative outcome:</strong> Development felt accelerated with comprehensive AI assistance</li>
                 </ul>

                 <h3>Architecture Patterns That Worked</h3>
                 <ul>
                     <li>Dual loop model: Human + AI feedback cycles</li>
                     <li>YAML-driven prompt templates with version control</li>
                     <li>AI-writable config with human-locked critical fields</li>
                     <li>Self-scaffolding CLI + comprehensive observability layer</li>
                 </ul>

                 <h3>Testing & QA Approach</h3>
                 <ul>
                     <li>Test-first scaffolds with AI-generated test shells</li>
                     <li>Focus on edge cases and boundary conditions</li>
                     <li>Logs-as-verification for dynamic AI behavior</li>
                     <li>Human-in-the-loop QA checklist for all AI outputs</li>
                 </ul>

                 <h3>The Agentic Playbook Summary</h3>
                 <ul>
                     <li><strong>5 principles:</strong> AI is a tool, design for failure, human-in-loop mandatory</li>
                     <li><strong>5 things to build:</strong> versioned prompts, config schemas, observability</li>
                     <li><strong>5 things to avoid:</strong> opaque agents, destructive operations, no QA</li>
                 </ul>

                 <h3>What Still Needs Research</h3>
                 <ul>
                     <li>Quantitative productivity benchmarks across different project types</li>
                     <li>Prompt versioning and agent performance tracking standards</li>
                     <li>Long-term maintainability patterns for hybrid human/AI systems</li>
                 </ul>
             </div>
         </section>

        <!-- Back to Main Site -->
        <div class="back-to-main">
            <a href="index.html#services" class="cta-button">
                ← Back to Services
            </a>
        </div>
    </main>

    <!-- Footer -->
    <footer class="footer">
        <div class="container">
            <p class="footer-text">© 2025 Small Batch Maps. This case study demonstrates real-world agentic development practices.</p>
        </div>
    </footer>

    <!-- Import main site's JavaScript -->
    <script src="script.js"></script>
    
    <!-- Case Study specific JavaScript -->
    <script>
        // Smooth scroll for case study navigation
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
        
        // Highlight current section in navigation
        function highlightCurrentSection() {
            const sections = document.querySelectorAll('.part-section');
            const navLinks = document.querySelectorAll('.nav-links a');
            
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop - 150;
                if (window.pageYOffset >= sectionTop) {
                    current = '#' + section.getAttribute('id');
                }
            });
            
            navLinks.forEach(link => {
                link.classList.remove('active');
                if (link.getAttribute('href') === current) {
                    link.classList.add('active');
                }
            });
        }
        
        window.addEventListener('scroll', highlightCurrentSection);
        window.addEventListener('load', highlightCurrentSection);
    </script>
</body>
</html> 